# 7장 카프카와 Kafka Connect로 데이터 허브 구축하기

## 7.1 이 장의 내용

5장에서 카프카의 실제 사례를 소개했다. 여기서는 카프카의 전형적인 사례 중 하나인 데이터 허브 아키텍처를 응용한 사례를 살펴본다. 5장에선 소개한 대로 데이터 허브 아키텍처의 기본 개념은 데이터를 여러 시스템에 전달시키는 것이다. 카프카와 Kafka Connect를 사용하여 데이터 허브를 구축하고 체험하면서 그 개념을 이해하자.

## 7.2 Kafka Connect란

Kafka Connect는 아파치 카프카에 포함된 프레임워크로 카프카와 다른 시스템과의 데이터 연계에 사용한다. 카프카에 데이터를 넣거나, 카프카에서 데이터를 추출하는 과정을 간단히 하기 위해 만들어졌다. 6장에서 설명한 카프카의 데이터 파이프라인에서 포변 Kafka Connect는 프로듀서와 컨수머 양쪽 모두를 구성할 수 있다. 카프카는 특성상 다양한 시스템과 연계하기 때문에 Kafka Connect에서는 다른 시스템과 연결하는 부분은 커넥터라는 플러그인으로 구현하는 방식을 취하고 있으며, 'Kafka Connect 본체 + 플러그인' 구성으로 동작한다. Kafka Connect에서는 카프카에 데이터를 넣는 프로듀서 쪽 커넥터를 소스라고 부르고, 카프카에 데이터를 출력하는 컨수머 쪽의 커넥터를 싱크라고 한다.

이미 많은 플러그인이 공개되어 있어 연결할 곳에 맞는 커넥터가 있으면 직접 코딩하지 않더라도 데이터 입출력을 실행할 수 있다. 컨플루언트 공식 사이트에는 공개된 커넥터 목록을 볼 수 있다. 이 사이트에서 컨플루언트 플랫폼에 포함되어 있는 커넥터나 컨플루언트에 인정받은 커넷터, 기타 공개된 커넥터까지 여러 종류의 다양한 커넥터를 찾을 수 있다.

이러한 커넥터를 사용하면 다양한 시스템을 연결할 수 있다. 단, 주의 사항이 있다. 모든 플러그인이 소스와 싱크를 구현하고 있지는 않다는 점이다. 소스나 싱크 둘 중 하나만 제공하는 플러그인도 많기 때문에 확인이 필요하다. 또한 공개 플러그인이 반드시 카프카 커뮤니티와 컨플루언트에 의해 개발된 것이 아니기 때문에 품질이나 문서의 총실도에 차이가 있을 수 있는 점은 유의해야 한다.

사용하려는 커넥터가 없을 경우 직접 커넥터를 구현할 수 있다. 이 책에서는 다루지 않지만 컨플루언트에서 제공하는 커넥터 개발 가이드를 참고하여 만들 수 있으므로 흥미 있는 사람은 도전해보길 바란다.

Kafka Connect는 데이터를 카프카에 입출력하기 위한 확장 가능한 아키텍처를 갖고 있으며 여러 서버로 클러스터를 구성할 수 있다. Kafka Connect는 브로커와 동일한 서버에 동작할 수 있어 카프카 클러스트와 Kafka Connect 클러스터를 함께 구성하는 것도 가능하다. Kafka Connect 클러스터에서 소스나 싱크의 플러그인으로 데이터를 입출력할 때는 논리적인 작업을 실행한다. 이 논리적인 작업을 **커넥터 인스턴스**(또는 단순히 커넥터)라고 부른다. 커넥터 인스턴스는 여러 태스크를 실행하는데, 이 태스크는 클러스터 상의 여러 서버에서 잘 분담해서 실행되며 실제적인 데이터의 복사를 실시한다. 이처럼 Kafka Connect는 하나의 커넥터 인스턴스가 여러 태스크를 가질 수 있는 구조라서 확장 가능한 데이터 입출력이 가능하다.

## 7.3 데이터 허브 아키텍처 응용 사례

여기서부터는 Kafka Connect를 사용하여 데이터 허브 아키텍처를 구현하는 방식을 살펴본다. 데이터 허브는 여러 시스템 사이에서 데이터를 전달하기 위한 허브다. 따라서 데이터 허브를 사용할 때는 데이터를 연계하려는 여러 시스템도 함께 고려할 필요가 있다.

### 7.3.1 데이터 허브 아키텍처가 유효한 시스템

데이터 허브 아키텍처가 유효한 시스템은 무엇인지 어떤 대규모 소매점을 예로 살펴보자. 실제 매장이 전국에 있는 대형 소매점 A가 있다. A 회사는 점포의 재고 관리에 배송 관리, 포인트카드의 회원정보 관리, 판매 관리, POS 등 사업을 운영하기 위한 많은 시스템이 있다. 그뿐 아니라 인터넷 쇼핑몰도 판매 채널이 있으며, 직접 전사상거래 사이트를 운영하는 한편 대형 온라인 쇼핑몰에도 인터넷 매장을 열고 있다.

이 예에서 A 회사는 업무를 하기 위해 많은 시스템을 사용하고 있다. 이것은 A 회사에만 해당하는 이야기가 아니라 대기업과 중견기업이라면 모두 비슷한 IT 전산화를 추진하고 있을 것이다. 아마도 여러분 주변도 이미 이와 비슷한 상황이 아닐까 싶다.

A 회사에서의 데이터 전달에 대해 생각해보자. 만약 A 회사에서 다음달 출시되는 주력 상품의 판매 예측을 세워야 한다면 대응할 수 있을까? A 회사에는 실제 매장의 판매 관리 시스템이 있기 때문에 과거의 데이터도 있을 것이고, 전자상거래 사이트 또한 매출 데이터를 가지고 있을 것이다. 과거의 매출 데이터를 잘 조합해 포인트카드에서 소비자를 연관지어 분석할 수 있으면 판매예측이 가능할 것이다. 다른 예도 생각해보자. 만약 A 회사에서 주문 처리를 효율화하기 위해 자동주문을 하려고 하면 가능할까? A 회사는 과거 매출데이터와 판매예측 데이터도 있고 재고 데이터도 있다. 이것을 조합하면 자동 주문도 할 수 있을 것이다.

하지만 이것이 가능하려면 시스템 간 데이터를 잘 전달할 수 있다는 경우에 한해서다. 만약 실제 매장의 판매 관리 시스템의 데이터 형식이 각 점포와 차이가 있어 제대로 통합할 수 없다면 판매 예측은 어렵다. 만약 재고 관리 시스템이 자동 주문 시스템에 데이터를 전달하는 인터페이스가 없다면 그 이유만으로 자동 주문은 포기해야 한다. 장래에 실제 매장과 연계한 재고 정보를 전자상거래 사이트에 표시하려고 해도 이 또한 포기해야 한다. 바로 이것이 5장에서 설명한 사일로화의 폐해이다. 이렇듯 시스템이 많아지면 모든 시스템을 연결하는 것이 현실적으로 어려워진다.

새로운 자동 주문 시스템을 만들었을 때 데이터를 출력하는 쪽인 재고 관리 시스템에도 수정을 해야하는 상황이라면 연관 시스템이 다운될지도 모른다. 이러한 경우 데이터 허브 아키텍처를 사용한다면 시스템 다운은 일어나지 않는다. 왜냐하면 데이터 허브 아키텍처에서는 재고 관리 시스템은 데이터 허브에서만 데이터를 입출력하고, 판매 관리 시스템과 자동 주문 시스템도 이와 마찬가지로 데이터 허브에만 입출력을 실행하기 때문이다. 이렇게 하면 각각의 시스템 설계가 단순화되고 시스템끼리 자유롭게 데이터를 전달할 수 있다.

### 7.3.2 데이터 허브 도입 후의 모습

A 회사에 데이터 허브 아키텍처가 도입된다면 어떻게 될까? 이후에서는 데이터 절달의 몇 가지 형태를 고려해보기로 한다. 데이터 전달은 다음 3가지를 구현하는 것을 목표로 한다.

1. 전자상거래 사이트에 실제 매장의 재고 정보를 표시하기<br>
   재고 관리 시스템의 데이터를 전자상거래 사이트에 전달한다.
2. 매월 판매 예측하기<br>
   실제 매장의 POS 시스템 데이터와 전사상거래 사이트의 매출 데이터를 판매 예측 시스템에 전달한다.
3. 자동 주문하기<br>
   판매 예측 데이터와 재고 관리 시스템의 데이터를 자동 주문 시스템에 전달한다.

하나하나가 매우 단순하기 때문에 대략적으로 상상이 가능할 것이다. 데이터 허브를 이용하는 방식에서는 각 시스템의 데이터를 데이터를 개별적으로 연계하는 것이 아니라 그 사이에 데이터 허브를 끼워 넣는 식으로 구현한다. 이렇게 함으로써 3가지 기능을 간단히 구현할 수 있다.

이제 각각을 개별적으로 살펴보자. 이 장에서는 3개의 기능중 1.(재고 정보 표시)과 2.(월 판매 예측)를 설명한다. 세 번째 자동 주문은 1.과 2.에서 배운 지식을 응용하여 여러분이 구현해보기를 바란다. Kafka Connect로 시스템끼리 연결하는 것은 대부분 데이터 저장소 사이에서 연계하게 된다. 따라서 3개 기능을 실현하는 데이터 허브의 데이터 저장소를 정리하면 다음과 같다.(그림 생략)

1.에서는 재고 관리 시스템과 전자상거래 사이트를 연결한다. 처음에 기본을 확실히 익히는 것이 중요하다. Kafka Connect에서는 Hello World 같은 것은 없지만 기본을 이해하는 데 좋은 로컬 파일 연계용 커넥터가 있다. 이 커넥터는 Kafka Connect 의 로컬 파일 업데이트를 감시하여 카프카에 데이터를 넣거나, 카프카의 데이터를 Kafka Connect의 로컬 파일에 기록하는 커넥터다. 로컬 파일을 사용하기 때문에 Kafka Connect의 클러스터만 있으면 동작하므로 간편하게 사용할 수 있다. 재고 관리 시스템은 순차적으로 새로운 데이터를 파일에 추가해가며 전자상거래 사이트는 업데이트 데이터를 파일 형식으로 받는 식으로 시스템이 완성된다. 이 예제로 Kafka Connect의 기본을 파악하고 동작을 이해하는 것을 목표로 한다.

2.에서는 POS, 전자상거래 사이트, 판매 예측 시스템을 연결한다. 기본을 마쳤으니 실제로 있을 법한 경우를 체험해보자. 많은 시스템의 데이터 저장소 계층은 RDBMS인 경우가 많다. 여기서는 전사상거래 사이트나 POS가 RDBMS에 데이터를 저장하고 있다고 가정한다. 여러 시스템을 도입하고 있는 회사는 시스템마다 사용하는 RDBMS가 각각 다른 경우가 많다. 이번에도 전자상거래 사이트와 POS를 서로 다른 제품으로 시도해보자. 여기에서는 PostgreSQL과 MariaDB를 사용한다. 판매 예측 시스템은 최근 흔히 볼 수 있는 구성으로 클라우드 서비스로 구현한다고 가정해보자. 이 경우에는 클라우드 서비스의 스토리지에 데이터를 두는 것이 좋을 것이다. 여기에서는 AWS의 S3으로 가정한다. 이 예제를 통해 실제 Kafka Connect의 사용법을 파악할 수 있을 것이다.

3.은 독자가 생각할 차례다. 컨플루언트의 공식 사이트에 공개된 Kafka Connect 목록에서 생각하고 있는 시스템에서 사용할 만한 커넥터를 찾아 구현해보기를 바란다.

참고로 데이터 허브 아키텍처는 Kafka Connect를 사용하지 않고도 프로듀서나 컨수머를 개별적으로 구현할 수도 있다. 실제로 이번과 같은 사례에서는 어떠한 방식으로든지 구현할 수 있을 것이다. Kafka Connect를 사용하는 경우 이미 커넥터가 공개되어 있는 연결 시스템이라면 코딩 없이 비교적 간단하게 시스템끼리 연계할 수 있다는 점이 큰 장점이다. 단, 실제 구현의 난이도/성능 요구 사항/시맨틱스(어느정도의 메시지 손실과 중복을 허용할지) 등을 종합적으로 판단하여 결정한다.

그럼 Kafka Connect를 사용한 데이터 허브를 구축해보자.

## 7.4 환경 구성

데이터 허브를 구현하기 위해서는 준비가 필요하다. 당연한 이야기지만 데이터 허브는 데이터 허브 단독으로 존재해봐야 아무런 의미가 없기 때문에 접속할 시스템이 존재해야 한다. 필요한 준비 환경을 나열하고 전체 그림을 그려보자.

우선적으로 필요한 것은 카프카 클러스터와 Kafka Connect다. 카프카 클러스터는 3장에서 구축한 것이 있으면 그대로 사용할 수 있다. 만약에 없으면 3장의 절차에 따라 클러스터를 구축하면 된다. Kafka Connect는 카프카 클러스터의 브로커와 함께 설치해 실행한다.

다음으로 필요한 것은 연계 시스템이다. 1.에서는 재고관리 시스템과 전자상거래 사이트가 등장했다. 2.에서는 POS, 전자상거래 사이트, 판매 예측 시스템이 등장했다. 이러한 시스템을 준비해야 한다. 그렇다고 해도 이 예제를 위해 일부러 웹사이트나 애플리케이션, 배치 등을 만드는 일은 상당히 힘든 작업이다. 데이터 허브가 동작하는 방식을 체험하려면 연계에 필요한 데이터 저장소만 준비하면 된다. 데이터는 수작업을 통한 입력으로도 충분하다.

준비가 필요한 카프카 클러스터와 연계 시스템용 데이터 저장소를 아래와 같이 정리했다.

| 준비물              | 구성제품       | 서버명               | 비고                                   |
| ------------------- | -------------- | -------------------- | -------------------------------------- |
| 카프카 클러스터     | Kafka          | kafka-broker01,02,03 | 3대 구성                               |
| 1.재고관리 시스템   | 로컬파일시스템 | kafka-broker01       | 카프카 클러스터의 로컬 파일시스템 사용 |
| 2.전자상거래 사이트 | 로컬파일시스템 | kafka-broker01       | 카프카 클러스터의 로컬 파일시스템 사용 |
| 2.전자상거래 사이트 | PostgreSQL     | ec-data-server       |
| 2.POS               | MariaDB        | pos-data-server      |
| 2.판매예측 시스템   | S3             | 없음                 | AWS 사용                               |

준비가 필요한 것은 그 외에 더 있다. 이들을 연결하는 커넥터도 검토한다. Kafka Connect는 연결 시스템에 맞춰 커넥터 플러그인이 필요하다. 사용할 커넥터 플러그인을 선택해두자.

1.에서 파일 연계를 위해 필요한 커넥터는 다음과 같다.

- FileStream Connectors (Source)
- FileStream Connectors (Sink)

한편 2.의 경우 POS와 전자상거래 사이트로부터는 RDBMS의 데이터를 받고, 판매 예측 시스템에는 S3에 전달하기 때문에 사용하는 커넥터는 다음과 같다.

- JDBC Connector (Source)
- S3 Connector (Sink)

여기까지 이번에 사용할 환경이다. 그럼 순서대로 만들어보자.
